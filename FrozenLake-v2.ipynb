{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw_Wu-vTp0Yx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n",
        "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#new_step_api = True\n",
        "env = gym.make('FrozenLake-v1', desc=generate_random_map(size=8))\n",
        "new_step_api = True"
      ],
      "metadata": {
        "id": "e2s9RpgmrGlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "\n",
        "q_table = np.zeros((state_space_size,action_space_size))\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL3lXO_yqWSJ",
        "outputId": "f6ffb3e7-1fc7-40e5-bc7c-c6f2d1b20d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 10000 #total no of episodes our agent will play during the training period\n",
        "max_steps_per_episode = 100 #max no of steps our agent can take in each episode\n",
        "\n",
        "learning_rate = 0.1 #the amount of q value agent will take from the old q value for a specific (s,a) pair\n",
        "discount_rate = 0.99 #gamma in bellman optimal equation\n",
        "\n",
        "#exploration and exploitation tradeoffs\n",
        "exploration_rate = 1\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001"
      ],
      "metadata": {
        "id": "VW0R01QBrFis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_all_episodes = [] #used to hold all the rewards in each episode\n",
        "\n",
        "#Q-Learning algorithm\n",
        "for episode in range(num_episodes): #this loop contains everything that happens within a single episode\n",
        "  state,_ = env.reset() #set our env back to starting state for each episode, unpack the tuple to get state\n",
        "\n",
        "  done = False #checks whether our episode is finished\n",
        "  rewards_current_episode = 0 #stores the rewards within the current episode\n",
        "\n",
        "  for step in range(max_steps_per_episode): #contains everything that happens at a single time stamp within each episode\n",
        "\n",
        "    #Exploration-exploitation trade-off\n",
        "    exploration_rate_threshold = random.uniform(0,1) #this value will determine if our agent will explore or exploit for this time step\n",
        "    if exploration_rate_threshold > exploration_rate: #explor - exploit trade off logic\n",
        "      action = np.argmax(q_table[state,:])   #this will return the max value in our current q values\n",
        "    else:\n",
        "      action = env.action_space.sample() #the agent will explore the env and sample an action randomnly\n",
        "\n",
        "    #after our action is taken we then will call the action with .step, which will pass our action to env. .step will return a tuple which contains\n",
        "    #the new state(new_state), the reward our agent got with that action, done(whether or not our action ended that episode or not) and info(which contains some more\n",
        "    # diagnostic information regarding our environment which can later be helpful for us to debug)\n",
        "    new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    #After analyzing reward which we got from our action, we will then update our q value for that (s,a) pair in the q table.\n",
        "    #Update Q-table for Q(s,a)\n",
        "    q_table[state,action] = q_table[state,action] * (1- learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
        "\n",
        "    state = new_state #now we will update our current state with the new state which we got before\n",
        "    rewards_current_episode += reward #we will also update our reward by adding it to the previous rewards(rewards_current_episode)\n",
        "\n",
        "    if done == True: #this will check if our agent reached to its desired state in this episode\n",
        "       break\n",
        "\n",
        "  # Exploration rate decay (Once the episode is complete, we need to update our exploration rate by using exploration rate decay)\n",
        "  exploration_rate = min_exploration_rate +  (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
        "\n",
        "  rewards_all_episodes.append(rewards_current_episode) #append the reward from our current episode to the list which contains the rewards from all the episodes\n",
        "\n",
        "#Calculate and print the average reward per thousand episodes\n",
        "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
        "count = 1000\n",
        "print(\"*******Average reward per thousand episodes*******\\n\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "  print(count, \": \", str(sum(r/1000)))\n",
        "  count += 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_29KQaNqzqZ",
        "outputId": "52b8b6f2-67d9-478f-edc5-4929466db745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******Average reward per thousand episodes*******\n",
            "\n",
            "1000 :  0.0\n",
            "2000 :  0.0\n",
            "3000 :  0.0\n",
            "4000 :  0.0\n",
            "5000 :  0.0\n",
            "6000 :  0.0\n",
            "7000 :  0.0\n",
            "8000 :  0.0\n",
            "9000 :  0.0\n",
            "10000 :  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print updated q table\n",
        "print(\"\\n\\n *******Q-table********\\n\")\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9TfpwLv4TjC",
        "outputId": "1a8ef564-0424-4770-b3ae-821697b7b10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " *******Q-table********\n",
            "\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Our agent will now play the game after the previous learning\n",
        "\n",
        " for episode in range(3): #taking range as 3 means we will go through 3 episodes\n",
        "     state,info = env.reset() #this will reset the environment from scratch for the next episode in loop, unpack the tuple\n",
        "     done = False #will be used to prompt weather the agent has reached or not\n",
        "     print(\"*****EPISODE \", episode + 1, \"*****\\n\\n\\n\\n\")\n",
        "     time.sleep(1) #This is to give us time to read the output before it disappears from the screen.\n",
        "\n",
        "     for step in range(max_steps_per_episode): #this loop is for each time step\n",
        "         clear_output(wait=True) #this will clear the last output from the notebook cell for clarity. by setting wait as true, it waits for the program to clear before it writes another output.\n",
        "         gym.make('FrozenLake-v1', desc=generate_random_map(size=8))#this will render the environment into the screen so that we can see what is happening\n",
        "         time.sleep(1) #we will see the rendered env for 900ms on screen\n",
        "\n",
        "         action = np.argmax(q_table[state,:]) #set action which has the highest q value for the current state\n",
        "         new_state, reward, terminated, truncated, info = env.step(action) #as discussed above, action returns 3 values.\n",
        "         done = terminated or truncated #check if the episode is done\n",
        "\n",
        "\n",
        "         if done:\n",
        "             clear_output(wait=True)\n",
        "             gym.make('FrozenLake-v1', desc=generate_random_map(size=8))\n",
        "             if reward == 1:\n",
        "                 print(\"*****YOU REACHED THE GOAL!!\")\n",
        "                 time.sleep(3)\n",
        "             else:\n",
        "                 print(\"*****You fell into a hole!*****\")\n",
        "                 time.sleep(3)\n",
        "\n",
        "             break # Break out of the loop when the episode is done\n",
        "\n",
        "         state = new_state #After the episode is done, we will move our current state to the new state given by the action.\n",
        "\n",
        " env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVS5Z86e6B0L",
        "outputId": "e5db6df1-2aeb-41b3-9148-5e81404d40a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****You fell into a hole!*****\n"
          ]
        }
      ]
    }
  ]
}